{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55dbba56",
   "metadata": {},
   "source": [
    "# GPU 训练\n",
    "\n",
    "```{note}\n",
    "在 GPU 上训练大语言模型可以显著提高训练速度。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6cb98",
   "metadata": {},
   "source": [
    "## 环境配置\n",
    "\n",
    "我的系统是 windows 11，显卡是 4060，在此之上我做了如下环境配置：\n",
    "1. 使用 wsl2 安装 ubuntu 24.04\n",
    "2. 更新显卡驱动\n",
    "3. 在 ubuntu 系统下安装 conda，创建了一个新的环境，Python 版本是 3.13.9，安装了对应 PyTorch 版本\n",
    "4. 为了方便开发，安装了 windows 版本的 Trae，然后用 wsl 连上 ubuntu 系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48affaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch版本: 2.10.0+cu130\n",
      "CUDA版本: 13.0\n",
      "cuDNN版本: 91501\n",
      "GPU是否可用: True\n",
      "GPU数量: 1\n",
      "当前GPU索引: 0\n",
      "GPU名称: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"PyTorch版本:\", torch.__version__)\n",
    "print(\"CUDA版本:\", torch.version.cuda)  # 关键：这是PyTorch实际使用的CUDA版本\n",
    "print(\"cuDNN版本:\", torch.backends.cudnn.version())\n",
    "print(\"GPU是否可用:\", torch.cuda.is_available())\n",
    "print(\"GPU数量:\", torch.cuda.device_count())\n",
    "print(\"当前GPU索引:\", torch.cuda.current_device())\n",
    "print(\"GPU名称:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6bc450",
   "metadata": {},
   "source": [
    "## 使用 GPU 训练\n",
    "\n",
    "要使用 GPU 训练，需要：\n",
    "1. 模型放到 GPU 里：`model = SimpleNextTokenModel(vocab_size, embed_dim, hidden_dim).to(device)`\n",
    "2. 数据放到 GPU 里：训练时 `batch.to(device)`，预测时 `test_input = train_data[0][:, :-1].to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f4f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda\n",
      "模型结构初始化完成。\n",
      "开始训练，共 100 个 batch...\n",
      "Step [10/100], Loss: 6.9229\n",
      "Step [20/100], Loss: 6.9222\n",
      "Step [30/100], Loss: 6.9219\n",
      "Step [40/100], Loss: 6.9241\n",
      "Step [50/100], Loss: 6.9204\n",
      "Step [60/100], Loss: 6.9230\n",
      "Step [70/100], Loss: 6.9176\n",
      "Step [80/100], Loss: 6.9225\n",
      "Step [90/100], Loss: 6.9193\n",
      "Step [100/100], Loss: 6.9180\n",
      "训练结束！\n",
      "训练耗时: 0.62 秒\n",
      "\n",
      "验证第一个 batch 的预测:\n",
      "输入 shape: torch.Size([32, 128])\n",
      "预测 shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNextTokenModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 1. Embedding 层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. MLP 层\n",
    "        # 简单结构: Linear -> Activation -> Linear\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # 3. 输出层 (Linear)\n",
    "        # 将维度从 embed_dim 映射回 vocab_size\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            input_ids: (batch_size, sequence_length)\n",
    "        返回:\n",
    "            logits: (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # logits shape: (batch_size, seq_len, vocab_size)\n",
    "        logits = self.output_head(x)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def generate_dummy_data(num_batches, batch_size, seq_len, vocab_size):\n",
    "    \"\"\"生成简单的随机数据\"\"\"\n",
    "    data = []\n",
    "    for _ in range(num_batches):\n",
    "        # 随机生成数据\n",
    "        batch = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        data.append(batch)\n",
    "    return data\n",
    "\n",
    "\n",
    "# 1. 设置随机种子，保证可复现性\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 自动检测设备\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"使用设备: {device}\")\n",
    "\n",
    "# 2. 超参数\n",
    "vocab_size = 1000\n",
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "seq_len = 128   # 序列长度\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_batches = 100 # 训练步数\n",
    "\n",
    "# 3. 初始化模型并移动到设备\n",
    "model = SimpleNextTokenModel(vocab_size, embed_dim, hidden_dim).to(device)\n",
    "print(\"模型结构初始化完成。\")\n",
    "\n",
    "# 4. 定义 Loss 和 Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 5. 准备虚拟数据\n",
    "# 注意：这里的 data 包含输入和目标，所以 seq_len + 1\n",
    "train_data = generate_dummy_data(num_batches, batch_size, seq_len + 1, vocab_size)\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"开始训练，共 {num_batches} 个 batch...\")\n",
    "model.train()\n",
    "\n",
    "for step, batch in enumerate(train_data):\n",
    "    # 将数据移动到设备\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # 构造输入和目标 (Next Token Prediction)\n",
    "    # 输入: 序列的前 N-1 个 token\n",
    "    # 目标: 序列的后 N-1 个 token (即每个位置的下一个 token)\n",
    "    input_ids = batch[:, :-1]  # (B, T)\n",
    "    targets = batch[:, 1:]     # (B, T)\n",
    "    \n",
    "    # 清零梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播\n",
    "    logits = model(input_ids) # (B, T, V)\n",
    "    \n",
    "    # 计算 Loss\n",
    "    # CrossEntropyLoss 需要 (N, C) 和 (N) 的输入\n",
    "    # logits.view(-1, vocab_size) -> (B*T, V)\n",
    "    # targets.view(-1) -> (B*T)\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.reshape(-1))\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step [{step+1}/{num_batches}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"训练结束！\")\n",
    "end_time = time.time()\n",
    "print(f\"训练耗时: {end_time - start_time:.2f} 秒\")\n",
    "\n",
    "# 简单验证一下\n",
    "test_input = train_data[0][:, :-1].to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(test_input)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    print(\"\\n验证第一个 batch 的预测:\")\n",
    "    print(f\"输入 shape: {test_input.shape}\")\n",
    "    print(f\"预测 shape: {preds.shape}\")\n",
    "    # 这里只是随机数据，预测准确率不会高，主要是验证流程跑通\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0b091d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
