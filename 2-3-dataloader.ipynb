{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23aaaaa5",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "\n",
    "```{note}\n",
    "`Dataset` 和 `DataLoader` 是 PyTorch 数据处理的核心组件。简单来说：\n",
    "\n",
    "*   Dataset：负责 “存” 和 “取”。它定义了数据在哪里，以及如何取出一个样本（通常通过索引）。\n",
    "*   DataLoader：负责 “运”。它从 Dataset 里批量取出数据，打成包 (Batch)，可以顺便做打乱 (Shuffle) 和多进程加速。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b755",
   "metadata": {},
   "source": [
    "## DataSet\n",
    "\n",
    "为了直观理解，我们结合刚才编写的 `BPETokenizer` 来理解 `Dataset`。\n",
    "\n",
    "我们需要继承 `torch.utils.data.Dataset` 并实现两个方法：\n",
    "- `__len__`: 告诉 PyTorch 数据集有多大。\n",
    "- `__getitem__`: 告诉 PyTorch 第 `idx` 个样本长什么样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4352ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "class PretrainDataset(Dataset):\n",
    "    def __init__(self, file_paths, tokenizer, seq_len, text_col='page'):\n",
    "        \"\"\"\n",
    "        :param file_paths: Parquet 文件路径列表\n",
    "        :param tokenizer: 分词器 (需要包含 <EOS> 和 <PAD>)\n",
    "        :param seq_len: 训练序列长度 (Context Window)\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 获取特殊 token ID\n",
    "        self.eos_id = tokenizer.special_tokens.get(\"<EOS>\")\n",
    "        self.pad_id = tokenizer.special_tokens.get(\"<PAD>\")\n",
    "        \n",
    "        if self.eos_id is None:\n",
    "            raise ValueError(\"Tokenizer must have <EOS> defined for pretraining.\")\n",
    "        if self.pad_id is None:\n",
    "            print(\"Warning: <PAD> not found in tokenizer. Using 0 as pad_id.\")\n",
    "            self.pad_id = 0\n",
    "            \n",
    "        print(f\"Processing {len(file_paths)} files...\")\n",
    "        \n",
    "        # 1. 读取并编码所有文本\n",
    "        texts = []\n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                df = pd.read_parquet(file_path)\n",
    "                if text_col in df.columns:\n",
    "                    texts.extend(df[text_col].dropna().tolist())\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        # 为了增加随机性，可以在拼接前打乱文本顺序\n",
    "        random.shuffle(texts)\n",
    "        \n",
    "        all_token_ids = []\n",
    "        for text in texts:\n",
    "            ids = tokenizer.encode(text)\n",
    "            all_token_ids.extend(ids)\n",
    "            # 每条数据后加 EOS\n",
    "            all_token_ids.append(self.eos_id)\n",
    "            \n",
    "        # 转为 Tensor 存储\n",
    "        self.data = torch.tensor(all_token_ids, dtype=torch.long)\n",
    "        \n",
    "        # 计算样本总数\n",
    "        self.num_samples = math.ceil(len(self.data) / self.seq_len)\n",
    "        print(f\"Total tokens: {len(self.data)}. Total samples (seq_len={seq_len}): {self.num_samples}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 计算切片范围\n",
    "        start = idx * self.seq_len\n",
    "        end = min(start + self.seq_len, len(self.data))\n",
    "        \n",
    "        chunk = self.data[start:end]\n",
    "        \n",
    "        # 如果长度不足 seq_len，进行 Padding\n",
    "        if len(chunk) < self.seq_len:\n",
    "            pad_len = self.seq_len - len(chunk)\n",
    "            padding = torch.full((pad_len,), self.pad_id, dtype=torch.long)\n",
    "            chunk = torch.cat([chunk, padding])\n",
    "            \n",
    "        return chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983020e4",
   "metadata": {},
   "source": [
    "## IterableDataset\n",
    "          \n",
    "面对超大文件或者海量数据，不要一次性读入内存。你需要切换到 PyTorch 的 `IterableDataset`，并采用流式读取 (Streaming)的方式。\n",
    "\n",
    "这里有三个核心调整点：\n",
    "\n",
    "1.  继承 `IterableDataset`：\n",
    "    *   不要实现 `__getitem__`（因为它暗示随机访问，需要知道总长度）。\n",
    "    *   实现 `__iter__`，使用 Python 的 `yield` 关键字逐行/逐块“吐”出数据。\n",
    "\n",
    "2.  文件切分 (Sharding)：\n",
    "    *   在多进程 (`num_workers > 0`) 模式下，如果不做处理，每个 Worker 都会从头读取所有文件，导致数据重复。\n",
    "    *   必须在 `__iter__` 中获取 `torch.utils.data.get_worker_info()`，根据 Worker ID 分配不同的文件子集。\n",
    "\n",
    "3.  Shuffle 的变化：\n",
    "    *   `DataLoader(shuffle=True)` 对 IterableDataset **无效**。\n",
    "    *   解决方案：维护一个内存缓冲区 (Buffer)，先读入比如 10000 条数据，在缓冲区内随机打乱，然后 yield 出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class IterablePretrainDataset(IterableDataset):\n",
    "    def __init__(self, file_paths, tokenizer, seq_len):\n",
    "        \"\"\"\n",
    "        :param file_paths: Parquet 文件路径列表\n",
    "        :param tokenizer: 分词器 (需要包含 <EOS> 和 <PAD>)\n",
    "        :param seq_len: 训练序列长度 (Context Window)\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.seq_len = seq_len\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 获取特殊 token ID\n",
    "        self.eos_id = tokenizer.special_tokens.get(\"<EOS>\")\n",
    "        self.pad_id = tokenizer.special_tokens.get(\"<PAD>\")\n",
    "        \n",
    "        if self.eos_id is None:\n",
    "            raise ValueError(\"Tokenizer must have <EOS> defined for pretraining.\")\n",
    "        if self.pad_id is None:\n",
    "            print(\"Warning: <PAD> not found in tokenizer. Using 0 as pad_id.\")\n",
    "            self.pad_id = 0\n",
    "            \n",
    "    def __iter__(self):\n",
    "        worker_info = get_worker_info()\n",
    "        if worker_info is None:\n",
    "            # 单进程模式，处理所有文件\n",
    "            my_files = self.file_paths\n",
    "        else:\n",
    "            # 多进程模式，按 worker_id 分配文件\n",
    "            # 简单的 stride 分配: file[i] 分给 worker[i % num_workers]\n",
    "            num_workers = worker_info.num_workers\n",
    "            worker_id = worker_info.id\n",
    "            my_files = [f for i, f in enumerate(self.file_paths) if i % num_workers == worker_id]\n",
    "            \n",
    "        # 可以在文件级别 shuffle (如果需要的话)\n",
    "        # random.shuffle(my_files)\n",
    "        \n",
    "        buffer = []\n",
    "        \n",
    "        for file_path in my_files:\n",
    "            try:\n",
    "                # 使用 pyarrow 流式读取 Parquet\n",
    "                parquet_file = pq.ParquetFile(file_path)\n",
    "                \n",
    "                # 每次读取一个行组 (Row Group) 或者指定 batch_size\n",
    "                # batch_size=1000 行\n",
    "                for batch in parquet_file.iter_batches(batch_size=1000):\n",
    "                    df = batch.to_pandas()\n",
    "                    \n",
    "                    # 查找文本列\n",
    "                    if \"page\" in df.columns:\n",
    "                        texts = df[\"page\"].dropna().tolist()\n",
    "                    elif \"text\" in df.columns:\n",
    "                        texts = df[\"text\"].dropna().tolist()\n",
    "                    else:\n",
    "                        continue # 跳过没有文本列的 batch\n",
    "                        \n",
    "                    for text in texts:\n",
    "                        # Tokenize\n",
    "                        ids = self.tokenizer.encode(text)\n",
    "                        ids.append(self.eos_id) # 添加 EOS\n",
    "                        buffer.extend(ids)\n",
    "                        \n",
    "                        # 当 buffer 足够切分时，yield 出来\n",
    "                        while len(buffer) >= self.seq_len:\n",
    "                            chunk = buffer[:self.seq_len]\n",
    "                            buffer = buffer[self.seq_len:]\n",
    "                            yield torch.tensor(chunk, dtype=torch.long)\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {file_path}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        # 处理剩余的 buffer (如果不为空)\n",
    "        # 这里选择 yield 出来，交给 collate_fn 去 padding\n",
    "        if len(buffer) > 0:\n",
    "            yield torch.tensor(buffer, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498757f",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "我们使用 `PretrainDataset` 为例，演示如何使用 DataLoader 加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32c94a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_tokenizer import BPETokenizer\n",
    "\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load('wiki-tokenizer-1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f25006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 files...\n",
      "Total tokens: 266833. Total samples (seq_len=256): 1043\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "file_paths = ['data/wikitext-103-raw-v1-validation.parquet']\n",
    "dataset = PretrainDataset(file_paths, tokenizer, seq_len=256, text_col='page')\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=2,      # 每次取 2 个句子\n",
    "    shuffle=True,      # 打乱顺序\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca6d8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration...\n",
      "\n",
      "--- Batch 0 ---\n",
      "Shape: torch.Size([2, 256])\n",
      "Decoded (1st sample): ' ) , this caused concern among German government officials and clergy over data security and the potential for espionage . To assuage these concerns , Microsoft Germany agreed to provide a means to disable the utility . Following letters of complaint about discrimination from Scientology lawyers , some American companies such as General Electric , IBM and Ford Motor Company instructed their German subsidiaries to cease the use of protective declarations . \n",
      " The city @-@ state of Hamburg set up a full @-@ time office dedicated to opposing Scientology , the Scientology Task Force for the Hamburg Interior Authority , under the leadership of Ursula Caberta . In 2005 , in a case brought by a Scientologist , the Federal Administrative Court of Germany ordered the city of Hamburg to cease recommending the use of protective declarations to its business community , finding that the practice infringed religious freedom . In June 2008 , the Hamburg Administrative Court fined the city of Hamburg 5 @,@ 000 Euros ( $ 7 @,@ 000 ) for not complying with court instructions banning the use of \" sect filters . \" Internet links to sample filters to be used by businesses had continued'\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting iteration...\")\n",
    "for batch_idx, batch in enumerate(dataloader):\n",
    "    print(f\"\\n--- Batch {batch_idx} ---\")\n",
    "    print(f\"Shape: {batch.shape}\")\n",
    "    # print(f\"Data:\\n{batch}\")\n",
    "    \n",
    "    # 简单的解码演示 (只解码第一个样本，忽略 padding 0)\n",
    "    # 注意：我们的 tokenizer decode 需要 list[int]\n",
    "    first_sample_ids = batch[0].tolist()\n",
    "    # 过滤掉 padding (0)\n",
    "    valid_ids = [i for i in first_sample_ids if i != 0]\n",
    "    print(f\"Decoded (1st sample): '{tokenizer.decode(valid_ids)}'\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c09303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
