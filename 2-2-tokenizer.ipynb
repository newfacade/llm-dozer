{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "```{note}\n",
    "在训练 LLM 之前，我们需要处理的第一步就是 Tokenization。模型无法直接理解文本，它只能处理数字。Tokenizer 的作用就是把文本变成数字序列。<BR/>\n",
    "目前主流 LLM 几乎都使用 BPE (Byte Pair Encoding) 及其变体作为分词算法。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bpe-algo",
   "metadata": {},
   "source": [
    "## BPE (Byte Pair Encoding) 算法\n",
    "\n",
    "BPE 最初是一种数据压缩算法，后来被引入 NLP 领域用于分词。\n",
    "\n",
    "### 核心思想\n",
    "**“频率最高的相邻字符对，应该被合并成一个新的 token。”**\n",
    "\n",
    "它从字符级别开始，不断合并出现频率最高的“字符对”，直到达到预设的词表大小（Vocabulary Size）。\n",
    "\n",
    "### 算法流程\n",
    "\n",
    "1.  **准备语料**：准备大规模的训练文本。\n",
    "2.  **初始化**：把每个单词拆分成字符序列。例如 `\"hug\"` -> `[\"h\", \"u\", \"g\"]`。初始词表包含所有基础字符。\n",
    "3.  **统计频率**：统计所有相邻字符对（Bigram）在语料中出现的频率。\n",
    "4.  **合并 (Merge)**：找到频率最高的字符对（例如 `\"u\"` 和 `\"g\"` 经常一起出现），将它们合并成一个新的 token `\"ug\"`。并把语料中所有的 `\"u\", \"g\"` 替换为 `\"ug\"`。\n",
    "5.  **迭代**：重复步骤 3 和 4，直到词表大小达到预设值（例如 32000 或 100000）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "why-bpe",
   "metadata": {},
   "source": [
    "## 为什么 LLM 偏爱 BPE？\n",
    "\n",
    "### 1. 解决 OOV (Out of Vocabulary) 问题\n",
    "传统的按词分词（Word-based）如果遇到没见过的词（例如 `\"ChatGPT\"`），只能标记为 `<UNK>` (Unknown)，丢失信息。\n",
    "BPE 遇到生僻词时，会自动退化为子词甚至字符。例如 `\"ChatGPT\"` 可能被拆解为 `[\"Chat\", \"G\", \"PT\"]`。只要基础字符在词表里，模型就能处理任何字符串，永远不会出现 `<UNK>`。\n",
    "\n",
    "### 2. 平衡词表大小与序列长度\n",
    "- **Character-based**（按字分）：词表很小（26字母+符号），但序列极长，模型计算量大，且难以学到词义。\n",
    "- **Word-based**（按词分）：序列短，但词表巨大（几十万），稀疏性严重，且容易 OOV。\n",
    "- **Subword-based (BPE)**：折中方案。常见词是一个 token（如 `\"apple\"`），生僻词拆成多个 token（如 `\"apple\"`, `\"sauce\"`）。既保证了高频词的语义完整性，又控制了词表大小。\n",
    "\n",
    "### 3. 适应多语言\n",
    "BPE 不需要像中文分词那样依赖复杂的语法规则。它纯粹基于统计，因此可以把中文、英文、代码、数学公式混在一起训练，自动发现跨语言的通用结构（例如 HTML 标签 `<div>` 在任何语言语料里都是高频的，会被合并成一个 token）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cabf81d",
   "metadata": {},
   "source": [
    "## BPE 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e009bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {}  # (p0, p1) -> new_id\n",
    "        self.vocab = {}   # id -> bytes\n",
    "        self.special_tokens = {} # str -> id\n",
    "        self.inverse_special_tokens = {} # id -> str\n",
    "        # GPT-4 风格的正则表达式\n",
    "        # '(?:[sdmt]|ll|ve|re) 匹配常见缩写\n",
    "        # ?\\w+ 匹配单词\n",
    "        # ?\\d+ 匹配数字\n",
    "        # ?[^\\s\\w\\d]+ 匹配符号\n",
    "        # \\s+(?!\\S) 匹配尾部空格\n",
    "        # \\s+ 匹配其他空格\n",
    "        self.pattern = re.compile(r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\w+| ?\\d+| ?[^\\s\\w\\d]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def train(self, text, vocab_size, special_tokens=None):\n",
    "        \"\"\"\n",
    "        训练 BPE 分词器\n",
    "        :param text: 训练文本\n",
    "        :param vocab_size: 目标词表大小\n",
    "        :param special_tokens: 特殊 token 列表，如 [\"<unk>\", \"<pad>\"]\n",
    "        \"\"\"\n",
    "        print(f\"Training BPE Tokenizer with target vocab size: {vocab_size}...\")\n",
    "        \n",
    "        # 1. 预处理 Special Tokens 数量\n",
    "        if special_tokens is None:\n",
    "            special_tokens = []\n",
    "        num_special_tokens = len(special_tokens)\n",
    "        assert vocab_size >= 256 + num_special_tokens, \"Vocab size must be at least 256 + special_tokens\"\n",
    "        \n",
    "        # 计算 BPE 合并的目标 ID 上限\n",
    "        # 我们需要保留最后的 N 个 ID 给 Special Tokens\n",
    "        bpe_vocab_limit = vocab_size - num_special_tokens\n",
    "        \n",
    "        # 2. 预分词（Pre-tokenize）\n",
    "        # 将文本切分成单词块，防止跨单词合并，例如 \"dog.\" 中的 \"g\" 和 \".\" 不应该被合并\n",
    "        text_chunks = re.findall(self.pattern, text)\n",
    "        \n",
    "        # 3. 统计 Chunk 频率并转换为初始字节序列\n",
    "        chunk_counts = Counter(text_chunks)\n",
    "        \n",
    "        # ids_chunks: { \"chunk_str\": [byte_id1, byte_id2, ...] }\n",
    "        # 初始状态下，ID 就是 0-255 的字节值\n",
    "        ids_chunks = {chunk: [b for b in chunk.encode('utf-8')] for chunk in chunk_counts}\n",
    "        \n",
    "        # 初始化基础词表 (0-255)\n",
    "        for i in range(256):\n",
    "            self.vocab[i] = bytes([i])\n",
    "        \n",
    "        # 下一个可用的 ID (从 256 开始)\n",
    "        next_id = 256\n",
    "        \n",
    "        # 4. 迭代合并 (Training Loop)\n",
    "        num_merges = bpe_vocab_limit - 256\n",
    "        with tqdm(total=num_merges, desc=\"Training BPE\") as pbar:\n",
    "            while len(self.vocab) < bpe_vocab_limit:\n",
    "                # 统计当前所有 adjacent pairs 的频率\n",
    "                stats = Counter()\n",
    "                for chunk, freq in chunk_counts.items():\n",
    "                    ids = ids_chunks[chunk]\n",
    "                    for i in range(len(ids) - 1):\n",
    "                        pair = (ids[i], ids[i+1])\n",
    "                        stats[pair] += freq\n",
    "                \n",
    "                if not stats:\n",
    "                    print(\"No more pairs to merge. Stopping early.\")\n",
    "                    break\n",
    "                    \n",
    "                # 找到频率最高的 pair\n",
    "                # most_common(1) 返回 [(pair, count)]，取 [0][0] 得到 pair\n",
    "                best_pair = stats.most_common(1)[0][0]\n",
    "                \n",
    "                # 记录合并规则\n",
    "                self.merges[best_pair] = next_id\n",
    "                \n",
    "                # 更新词表：新 token 的字节序列 = 左 token 字节 + 右 token 字节\n",
    "                self.vocab[next_id] = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]\n",
    "                \n",
    "                # 在所有 chunks 中应用合并\n",
    "                # 这是一个简单的 O(N) 实现，效率一般但逻辑清晰\n",
    "                for chunk in ids_chunks:\n",
    "                    ids = ids_chunks[chunk]\n",
    "                    new_ids = []\n",
    "                    i = 0\n",
    "                    while i < len(ids):\n",
    "                        # 如果发现当前位置匹配 best_pair，则合并\n",
    "                        if i < len(ids) - 1 and ids[i] == best_pair[0] and ids[i+1] == best_pair[1]:\n",
    "                            new_ids.append(next_id)\n",
    "                            i += 2\n",
    "                        else:\n",
    "                            new_ids.append(ids[i])\n",
    "                            i += 1\n",
    "                    ids_chunks[chunk] = new_ids\n",
    "                \n",
    "                next_id += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # 5. 处理 Special Tokens (分配最后的 ID)\n",
    "        if special_tokens:\n",
    "            for token in special_tokens:\n",
    "                # 确保 special tokens 不会覆盖已有的 ID\n",
    "                # 注意：这里我们简单地将它们作为独立的 entry 加入词表\n",
    "                # 它们没有对应的 merges 规则，因为它们是不可分割的整体\n",
    "                self.vocab[next_id] = token.encode('utf-8')\n",
    "                self.special_tokens[token] = next_id\n",
    "                self.inverse_special_tokens[next_id] = token\n",
    "                next_id += 1\n",
    "                \n",
    "        print(f\"Training complete. Final vocab size: {len(self.vocab)}\")\n",
    "        print(f\"Special tokens map: {self.special_tokens}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将文本编码为 token ids\n",
    "        \"\"\"\n",
    "        # 1. 处理 Special Tokens\n",
    "        # 如果有 special tokens，我们需要先将它们从文本中切分出来，防止被 BPE 打碎\n",
    "        if not self.special_tokens:\n",
    "            special_pattern = None\n",
    "        else:\n",
    "            # 构造一个匹配任意 special token 的正则，注意要转义，排序是为了优先匹配更长的 token\n",
    "            sorted_specials = sorted(self.special_tokens.keys(), key=len, reverse=True)\n",
    "            special_pattern = re.compile(\"|\".join(re.escape(k) for k in sorted_specials))\n",
    "\n",
    "        # 最终的 ids 列表\n",
    "        ids = []\n",
    "        \n",
    "        # 辅助函数：对一段没有 special token 的纯文本进行 BPE 编码\n",
    "        def _encode_chunk(text_chunk):\n",
    "            if not text_chunk:\n",
    "                return []\n",
    "            \n",
    "            # 1. 预分词 (Regex split)\n",
    "            words = re.findall(self.pattern, text_chunk)\n",
    "            chunk_ids = []\n",
    "            \n",
    "            for word in words:\n",
    "                # 转为字节序列\n",
    "                word_bytes = [b for b in word.encode('utf-8')]\n",
    "                \n",
    "                # 2. BPE Merge\n",
    "                # 这里我们需要不断合并，直到无法合并为止\n",
    "                # 这是一个简单的实现：每次扫描所有可能的 pairs，找到在 merges 中最早出现的那个进行合并\n",
    "                while len(word_bytes) >= 2:\n",
    "                    # 找出当前序列中所有相邻的 pair\n",
    "                    stats = {}\n",
    "                    for i in range(len(word_bytes) - 1):\n",
    "                        pair = (word_bytes[i], word_bytes[i+1])\n",
    "                        # 检查这个 pair 是否在我们的合并规则中\n",
    "                        if pair in self.merges:\n",
    "                            stats[pair] = self.merges[pair] # 记录 pair -> new_id\n",
    "                    \n",
    "                    if not stats:\n",
    "                        break # 没有可以合并的 pair 了\n",
    "                    \n",
    "                    # 找到优先级最高（new_id 最小，即最早被 merge）的 pair\n",
    "                    # BPE 的合并顺序必须严格遵循训练时的顺序\n",
    "                    best_pair = min(stats, key=lambda p: self.merges[p])\n",
    "                    new_id = self.merges[best_pair]\n",
    "                    \n",
    "                    # 执行合并\n",
    "                    new_word_bytes = []\n",
    "                    i = 0\n",
    "                    while i < len(word_bytes):\n",
    "                        if i < len(word_bytes) - 1 and word_bytes[i] == best_pair[0] and word_bytes[i+1] == best_pair[1]:\n",
    "                            new_word_bytes.append(new_id)\n",
    "                            i += 2\n",
    "                        else:\n",
    "                            new_word_bytes.append(word_bytes[i])\n",
    "                            i += 1\n",
    "                    word_bytes = new_word_bytes\n",
    "                \n",
    "                chunk_ids.extend(word_bytes)\n",
    "            return chunk_ids\n",
    "\n",
    "        # 如果没有 special tokens，直接处理\n",
    "        if not special_pattern:\n",
    "            return _encode_chunk(text)\n",
    "\n",
    "        # 如果有 special tokens，我们需要切分\n",
    "        start = 0\n",
    "        for match in special_pattern.finditer(text):\n",
    "            # 处理前面的普通文本\n",
    "            non_special_text = text[start:match.start()]\n",
    "            if non_special_text:\n",
    "                ids.extend(_encode_chunk(non_special_text))\n",
    "            \n",
    "            # 处理 special token\n",
    "            special_token = match.group()\n",
    "            ids.append(self.special_tokens[special_token])\n",
    "            \n",
    "            start = match.end()\n",
    "        \n",
    "        # 处理剩余的文本\n",
    "        remaining_text = text[start:]\n",
    "        if remaining_text:\n",
    "            ids.extend(_encode_chunk(remaining_text))\n",
    "            \n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        将 token ids 解码为文本\n",
    "        \"\"\"\n",
    "        text_parts = []\n",
    "        current_bytes = []\n",
    "        \n",
    "        for idx in ids:\n",
    "            # 如果是 Special Token\n",
    "            if idx in self.inverse_special_tokens:\n",
    "                # 先把积攒的 bytes 解码并加入\n",
    "                if current_bytes:\n",
    "                    text_parts.append(b\"\".join(current_bytes).decode('utf-8', errors='replace'))\n",
    "                    current_bytes = []\n",
    "                # 加入 special token 字符串\n",
    "                text_parts.append(self.inverse_special_tokens[idx])\n",
    "            else:\n",
    "                # 如果是普通 Token，查表得到 bytes\n",
    "                # 注意：self.vocab[idx] 可能是单个字节，也可能是合并后的字节序列\n",
    "                if idx in self.vocab:\n",
    "                    current_bytes.append(self.vocab[idx])\n",
    "                else:\n",
    "                    # 未知 token (理论上不应该发生，除非 vocab 没对齐)\n",
    "                    pass\n",
    "        \n",
    "        # 处理最后剩余的 bytes\n",
    "        if current_bytes:\n",
    "            text_parts.append(b\"\".join(current_bytes).decode('utf-8', errors='replace'))\n",
    "            \n",
    "        return \"\".join(text_parts)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        将文本切分为 token 字符串列表，便于观察分词结果\n",
    "        对于无法解码为有效 UTF-8 的字节序列（如被切断的中文字符），将显示其字节表示（如 b'\\\\xe4'）\n",
    "        \"\"\"\n",
    "        ids = self.encode(text)\n",
    "        tokens = []\n",
    "        for idx in ids:\n",
    "            if idx in self.inverse_special_tokens:\n",
    "                tokens.append(self.inverse_special_tokens[idx])\n",
    "            elif idx in self.vocab:\n",
    "                token_bytes = self.vocab[idx]\n",
    "                try:\n",
    "                    # 尝试解码为字符串\n",
    "                    tokens.append(token_bytes.decode('utf-8'))\n",
    "                except UnicodeDecodeError:\n",
    "                    # 如果是无效的 utf-8 序列（比如被切断的多字节字符），显示其字节表示\n",
    "                    tokens.append(str(token_bytes))\n",
    "            else:\n",
    "                # Fallback for unknown ids\n",
    "                tokens.append(f\"<ID:{idx}>\")\n",
    "        return tokens\n",
    "\n",
    "    def save(self, file_path):\n",
    "        \"\"\"\n",
    "        保存模型到 JSON 文件\n",
    "        \"\"\"\n",
    "        # merges 的 key 是 tuple，JSON 不支持，转成 list 存储\n",
    "        # 格式: [ [p0, p1], new_id ]\n",
    "        merges_list = [[list(pair), new_id] for pair, new_id in self.merges.items()]\n",
    "        \n",
    "        model_data = {\n",
    "            \"merges\": merges_list,\n",
    "            \"special_tokens\": self.special_tokens\n",
    "        }\n",
    "        \n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(model_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        \"\"\"\n",
    "        从 JSON 文件加载模型\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            model_data = json.load(f)\n",
    "            \n",
    "        # 1. 恢复 merges\n",
    "        # JSON 里的 list 变成了 [ [p0, p1], new_id ]\n",
    "        self.merges = {tuple(pair): new_id for pair, new_id in model_data[\"merges\"]}\n",
    "        \n",
    "        # 2. 恢复 special_tokens\n",
    "        self.special_tokens = model_data[\"special_tokens\"]\n",
    "        self.inverse_special_tokens = {v: k for k, v in self.special_tokens.items()}\n",
    "        \n",
    "        # 3. 重建 vocab\n",
    "        self.vocab = {}\n",
    "        # 3.1 基础字符 (0-255)\n",
    "        for i in range(256):\n",
    "            self.vocab[i] = bytes([i])\n",
    "            \n",
    "        # 3.2 根据 merges 重建组合 token\n",
    "        # 必须按 new_id 从小到大顺序执行，因为后面的 token 可能依赖前面的\n",
    "        sorted_merges = sorted(self.merges.items(), key=lambda item: item[1])\n",
    "        for (p0, p1), new_id in sorted_merges:\n",
    "            self.vocab[new_id] = self.vocab[p0] + self.vocab[p1]\n",
    "            \n",
    "        # 3.3 恢复 special tokens 的 bytes\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            self.vocab[idx] = token.encode('utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ab4abe",
   "metadata": {},
   "source": [
    "## 训练和验证 BPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ad6776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>= Valkyria Chronicles III = \\n \\n Senjō no Va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= Tower Building of the Little Rock Arsenal =...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= Cicely Mary Barker = \\n \\n Cicely Mary Bark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>= Gambia women 's national football team = \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>= Plain maskray = \\n \\n The plain maskray or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29439</th>\n",
       "      <td>= Si Una Vez = \\n \\n \" Si Una Vez \" ( English...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29440</th>\n",
       "      <td>= Sicklefin lemon shark = \\n \\n The sicklefin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29441</th>\n",
       "      <td>= Flammulated flycatcher = \\n \\n The flammula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29442</th>\n",
       "      <td>= Ontario Highway 89 = \\n \\n King 's Highway ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29443</th>\n",
       "      <td>= Luke Smith ( writer ) = \\n \\n Luke Michael ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29444 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    page\n",
       "0       = Valkyria Chronicles III = \\n \\n Senjō no Va...\n",
       "1       = Tower Building of the Little Rock Arsenal =...\n",
       "2       = Cicely Mary Barker = \\n \\n Cicely Mary Bark...\n",
       "3       = Gambia women 's national football team = \\n...\n",
       "4       = Plain maskray = \\n \\n The plain maskray or ...\n",
       "...                                                  ...\n",
       "29439   = Si Una Vez = \\n \\n \" Si Una Vez \" ( English...\n",
       "29440   = Sicklefin lemon shark = \\n \\n The sicklefin...\n",
       "29441   = Flammulated flycatcher = \\n \\n The flammula...\n",
       "29442   = Ontario Highway 89 = \\n \\n King 's Highway ...\n",
       "29443   = Luke Smith ( writer ) = \\n \\n Luke Michael ...\n",
       "\n",
       "[29444 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('data/wikitext-103-raw-v1-train.parquet')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34426f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29444/29444 [00:00<00:00, 1130946.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "835072"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text = ''\n",
    "for i in tqdm(range(len(df))):\n",
    "    if random.random() < 0.001:\n",
    "        text += df.iloc[i].to_dict()['page'] + '\\n\\n'\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e0a067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE Tokenizer with target vocab size: 20000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training BPE: 100%|██████████| 19741/19741 [05:44<00:00, 57.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Final vocab size: 20000\n",
      "Special tokens map: {'<BOS>': 19997, '<EOS>': 19998, '<PAD>': 19999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "tokenizer.train(text=text, vocab_size=20000, special_tokens=['<BOS>', '<EOS>', '<PAD>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4a6e0cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = df.iloc[-1].to_dict()['page']\n",
    "tokenizer.decode(tokenizer.encode(page)) == page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9206b1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' =',\n",
       " ' Lu',\n",
       " 'ke',\n",
       " ' Smith',\n",
       " ' (',\n",
       " ' writer',\n",
       " ' )',\n",
       " ' =',\n",
       " ' \\n \\n',\n",
       " ' Lu',\n",
       " 'ke',\n",
       " ' Michael',\n",
       " ' Smith',\n",
       " ' is',\n",
       " ' an',\n",
       " ' American',\n",
       " ' writer',\n",
       " ' .',\n",
       " ' He',\n",
       " ' is',\n",
       " ' a',\n",
       " ' staff',\n",
       " ' member',\n",
       " ' at',\n",
       " ' B',\n",
       " 'ung',\n",
       " 'ie',\n",
       " ' ,',\n",
       " ' a',\n",
       " ' video']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(page)[: 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10443da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save('wiki-tokenizer-1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e573cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = BPETokenizer()\n",
    "x.load('wiki-tokenizer-1.json')\n",
    "x.tokenize(page) == tokenizer.tokenize(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d78996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
