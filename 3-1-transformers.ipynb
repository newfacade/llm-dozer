{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e84e962",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "```{note}\n",
    "Transformers æ˜¯ç”± Hugging Face å¼€å‘çš„ä¸€ä¸ªå¼€æº Python åº“ï¼Œç›®å‰è¢«è®¤ä¸ºæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ä¹ƒè‡³æ•´ä¸ªæ·±åº¦å­¦ä¹ é¢†åŸŸçš„äº‹å®æ ‡å‡†å·¥å…·ã€‚å®ƒçš„æ ¸å¿ƒç†å¿µæ˜¯è®©æ‰€æœ‰äººéƒ½èƒ½è½»æ¾ä½¿ç”¨æœ€å…ˆè¿›ï¼ˆSOTAï¼‰çš„æ¨¡å‹ã€‚å®ƒæœ‰ä¸‰å¤§æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "1. Tokenizerï¼š è´Ÿè´£æ•°æ®é¢„å¤„ç†ã€‚å®ƒå°†äººç±»è¯»å¾—æ‡‚çš„æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹è¯»å¾—æ‡‚çš„æ•°å­—ï¼ˆInput IDsï¼‰ã€‚\n",
    "2. Modelï¼š è´Ÿè´£å¤„ç†æ•°æ®ã€‚å®ƒæ˜¯ç¥ç»ç½‘ç»œçš„æœ¬ä½“ï¼ˆä¾‹å¦‚ BERT æˆ– GPT-2 çš„æ¶æ„ï¼‰ï¼Œè´Ÿè´£æ¥æ”¶æ•°å­—è¾“å…¥å¹¶è¾“å‡ºç»“æœã€‚\n",
    "3. Configï¼š è´Ÿè´£ç®¡ç†æ¨¡å‹çš„æ¶æ„å‚æ•°ï¼ˆå¦‚å±‚æ•°ã€éšè—å•å…ƒå¤§å°ç­‰ï¼‰ï¼Œç¡®ä¿æ¨¡å‹ç»“æ„æ­£ç¡®ã€‚\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9a31bc",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ Transformers\n",
    "\n",
    "### è”ç½‘ä½¿ç”¨\n",
    "\n",
    "ä»¥ Qwen3-0.6B ä¸ºä¾‹è¿›è¡Œè¯´æ˜ã€‚\n",
    "\n",
    "1. åŠ è½½ tokenizer å’Œ model\n",
    "    * å‰ææ¡ä»¶æ˜¯èƒ½è”ä¸Š hugging face\n",
    "    * ä¸‹é¢çš„ä»£ç å¦‚æœç¬¬ä¸€æ¬¡è¿è¡Œï¼Œä¼šä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œé»˜è®¤ä¼šä¸‹è½½åˆ° `~/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B`\n",
    "    * åç»­è¿è¡Œå¯èƒ½è¿˜æ˜¯éœ€è¦èƒ½è¿ hugging faceï¼Œæ¯”å¦‚æœ‰æœºåˆ¶å°è¯•è”ç½‘éªŒè¯æ˜¯å¦ä¸º Mistral æ¨¡å‹ã€‚ï¼ˆåé¢ä¼šè®²å¦‚æœè”ä¸ä¸Šè¯¥æ€ä¹ˆåŠï¼‰\n",
    "    * `torch_dtype=\"auto\"` å‘Šè¯‰åŠ è½½å™¨â€œ è¯·ä½¿ç”¨æ¨¡å‹åŸå§‹è®­ç»ƒæ—¶çš„ç²¾åº¦ â€æ¥åŠ è½½æƒé‡ã€‚Qwen3-0.6B åœ¨ `config.json` é‡Œè®°å½•äº†å®ƒæ˜¯ç”¨ bfloat16 è®­ç»ƒçš„ï¼Œæ‰€ä»¥ auto ä¼šè‡ªåŠ¨é€‰æ‹© bfloat16 ã€‚\n",
    "    * `device_map=\"auto\"` è‡ªåŠ¨è§„åˆ’æ¨¡å‹çš„æ¯ä¸€å±‚åº”è¯¥æ”¾åœ¨å“ªé‡Œï¼ˆGPU è¿˜æ˜¯ CPUï¼‰ã€‚ä¼˜å…ˆ GPUï¼Œå¦‚æœæ¨¡å‹å¤ªå¤§å®ƒä¼šè‡ªåŠ¨æŠŠæ”¾ä¸ä¸‹çš„å±‚åˆ‡åˆ†åˆ° CPU å†…å­˜ ä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "130fe321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3567314c348408f97e339cb7207baa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8732db",
   "metadata": {},
   "source": [
    "2. å‡†å¤‡è¾“å…¥æ•°æ®\n",
    "    * `tokenize=False` ä½¿å¾— text æ˜¯å­—ç¬¦ä¸²è€Œä¸æ˜¯ token ids\n",
    "    * `add_generation_prompt`\n",
    "        * False: è¿”å› '<|im_start|>user\\nWho are you?<|im_end|>\\n'\n",
    "        * True: è¿”å› '<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\n'\n",
    "    * `return_tensors=\"pt\"`: è¿”å› PyTorch å¼ é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3203a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nWho are you?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Who are you?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2c81d62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[151644,    872,    198,  15191,    525,    498,     30, 151645,    198,\n",
       "         151644,  77091,    198]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1b0cf",
   "metadata": {},
   "source": [
    "3. æ–‡æœ¬è¡¥å…¨\n",
    "    * `max_new_tokens=2048`: å³ output_ids çš„æœ€å¤§é•¿åº¦\n",
    "    * `skip_special_tokens=True`: è·³è¿‡ `<|im_end|>` ç­‰ç‰¹æ®Š tokenï¼Œä¸åŒ…æ‹¬ `<think>` å’Œ `</think>`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10d5b834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151667, 198, 32313, 11, 279, 1196, 4588, 11, 330, 15191]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=2048  # æ¼”ç¤ºç”¨ 2048 å³å¯\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "output_ids[: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47aa62c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content: <think>\n",
      "Okay, the user asked, \"Who are you?\" I need to respond appropriately. First, I should acknowledge their question. I can say something like, \"Hi there! I'm an AI assistant.\" Then, I should explain what I can do. Maybe mention helping with tasks like writing, solving problems, or providing information. It's important to keep the response friendly and open-ended so the user feels comfortable asking more questions. I should make sure the tone is helpful and not too technical. Let me put that together.\n",
      "</think>\n",
      "\n",
      "Hi! I'm an AI assistant designed to help with a wide range of tasks, from writing and problem-solving to providing information. How can I assist you today? ğŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "content = tokenizer.decode(output_ids, skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2c4a8a",
   "metadata": {},
   "source": [
    "### æœªè”ç½‘ä½¿ç”¨\n",
    "\n",
    "1. ä¹‹å‰å·²ç» load è¿‡ tokenizer å’Œ modelï¼Œç°åœ¨ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„è¿›è¡ŒåŠ è½½\n",
    "    * `os.path.expanduser` æŠŠè·¯å¾„é‡Œçš„æ³¢æµªå· ~ ç¿»è¯‘æˆ å®é™…çš„ç”¨æˆ·ä¸»ç›®å½•è·¯å¾„ \n",
    "    * `local_files_only=True` å‘½ä»¤ transformers åº“åªåœ¨æœ¬åœ°æ‰¾æ–‡ä»¶ï¼Œä¸è”ç½‘æ‰¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93753c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/newfacade/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# ä½¿ç”¨æœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œé¿å…è”ç½‘æ£€æŸ¥è§¦å‘ ConnectError\n",
    "model_name = os.path.expanduser(\"~/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/c1899de289a04d12100db370d81485cdf75e47ca\")\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9d2fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cdb2b1085494b50a7ed5f6d57aa6059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a094e2",
   "metadata": {},
   "source": [
    "2. å‡è®¾ä¹‹å‰æ²¡æœ‰ load è¿‡ï¼Œè¦æƒ³æ–¹æ³•ä¸‹è½½ä¸‹æ¥ç›¸å…³æ–‡ä»¶\n",
    "    * ä¸€ç§æ–¹æ³•æ˜¯ https://huggingface.co/Qwen/Qwen3-0.6B/tree/main ä¸­é€ä¸ªä¸‹è½½æ–‡ä»¶ï¼Œå†è½¬ç§»åˆ°å¼€å‘çš„ç›®å½•\n",
    "    * æ›´å¥½çš„æ–¹æ³•æ˜¯ clone æ•´ä¸ª repoï¼Œç›´æ¥ cloneï¼ˆclone ä¹Ÿéœ€è¦è”ç½‘ï¼Œè¿™é‡Œä»…è¿›è¡Œå±•ç¤ºï¼Œå®ƒè¿˜åœ¨ .git ç›®å½•å­˜å‚¨äº†æ‰€æœ‰æ–‡ä»¶çš„å‹ç¼©å‰¯æœ¬ï¼Œå ç”¨ç©ºé—´å‡ ä¹ç¿»å€ï¼‰ï¼š\n",
    "        ```bash\n",
    "        sudo apt update\n",
    "        sudo apt install git-lfs\n",
    "        git lfs install\n",
    "        git clone https://huggingface.co/Qwen/Qwen3-0.6B\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c6d444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b97bffe84bd44947acd9290b2508caf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/311 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tied weights mapping and config for this model specifies to tie model.embed_tokens.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n"
     ]
    }
   ],
   "source": [
    "# å‡è®¾ä¸‹è½½æˆ– clone ä¸‹æ¥çš„æ–‡ä»¶éƒ½æ”¾åœ¨æ­¤ç›®å½•ä¸‹\n",
    "model_name = '/home/newfacade/projects/Qwen3-0.6B'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    "    local_files_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271644aa",
   "metadata": {},
   "source": [
    "## æ–‡ä»¶ç»“æ„\n",
    "\n",
    "å…¸å‹çš„ transformers æ¨¡å‹ç›®å½•ç»“æ„å¦‚ä¸‹ï¼š\n",
    "\n",
    "```bash\n",
    "(base) newfacade@DESKTOP-R72H6BL:~/projects/Qwen3-0.6B$ du -sh *\n",
    "12K     LICENSE\n",
    "16K     README.md\n",
    "4.0K    config.json\n",
    "4.0K    generation_config.json\n",
    "1.6M    merges.txt\n",
    "1.5G    model.safetensors\n",
    "11M     tokenizer.json\n",
    "12K     tokenizer_config.json\n",
    "2.7M    vocab.json\n",
    "```\n",
    "\n",
    "1. æ¨¡å‹æƒé‡ model.safetensors\n",
    "2. æ¨¡å‹é…ç½®\n",
    "    * config.jsonï¼šæ¨¡å‹çš„â€œè¯´æ˜ä¹¦â€ ã€‚å®ƒå‘Šè¯‰ä»£ç è¿™æ˜¯ä¸€ä¸ª `Qwen3ForCausalLM` æ¶æ„çš„æ¨¡å‹ï¼Œæœ‰ 28 å±‚ç­‰ç­‰ã€‚\n",
    "    * generation_config.jsonï¼šç”Ÿæˆæ—¶çš„é»˜è®¤å‚æ•°ï¼Œæ¯”å¦‚ temperatureï¼ŒEOS token ID æ˜¯å¤šå°‘ç­‰ã€‚\n",
    "3. Tokenizer ç›¸å…³ï¼Œè¿™å‡ ä¸ªæ–‡ä»¶å…±åŒå†³å®šäº†å¦‚ä½•æŠŠâ€œæ–‡æœ¬â€å˜æˆâ€œæ•°å­— IDâ€\n",
    "    * tokenizer.jsonï¼š\n",
    "    * vocab.jsonï¼šè¯è¡¨æ˜ å°„ï¼Œå•çº¯çš„ token string -> ID çš„å­—å…¸ã€‚\n",
    "    * merges.txtï¼šBPE åˆå¹¶è§„åˆ™ï¼Œè®°å½•äº†å“ªäº›å­—ç¬¦å¯¹åº”è¯¥åˆå¹¶ï¼ˆæ¯”å¦‚ e + s -> es ï¼‰ã€‚\n",
    "    * tokenizer_config.jsonï¼šåˆ†è¯å™¨çš„é…ç½® ï¼Œæ¯”å¦‚ï¼šç”¨å“ªä¸ªç±»æ¥åŠ è½½ï¼ˆ Qwen2Tokenizer ï¼‰ï¼Œç‰¹æ®Šçš„ Token æ˜ å°„ã€‚\n",
    "4. å…¶ä»–æ–‡ä»¶\n",
    "    * LICENSE\n",
    "    * README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60e994",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
