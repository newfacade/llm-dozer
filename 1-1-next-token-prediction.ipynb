{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70e9f153",
   "metadata": {},
   "source": [
    "# Next Token Prediction Model\n",
    "\n",
    "```{note}\n",
    "如何训练一个工业级的大语言模型？其中的关键挑战和必要的背景知识又是哪些？我们的方法是一步步来，先从简单的模型和训练方法开始，然后逐步增加复杂度。<BR/>\n",
    "作为开头，本节我们定义一个简单的用于预测下一个token的模型，使用fake的数据，用Pytorch CPU跑通训练流程。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092838ba",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c705813-c276-4a5a-b26e-f35b8946fdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SimpleNextTokenModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # 1. Embedding 层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # 2. MLP 层\n",
    "        # 简单结构: Linear -> Activation -> Linear\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # 3. 输出层 (Linear)\n",
    "        # 将维度从 embed_dim 映射回 vocab_size\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        参数:\n",
    "            input_ids: (batch_size, sequence_length)\n",
    "        返回:\n",
    "            logits: (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.embedding(input_ids)\n",
    "        \n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        x = self.mlp(x)\n",
    "        \n",
    "        # logits shape: (batch_size, seq_len, vocab_size)\n",
    "        logits = self.output_head(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0f398da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNextTokenModel(\n",
       "  (embedding): Embedding(1000, 256)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "  )\n",
       "  (output_head): Linear(in_features=256, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 超参数\n",
    "vocab_size = 1000\n",
    "embed_dim = 256\n",
    "hidden_dim = 1024\n",
    "\n",
    "# 初始化模型\n",
    "model = SimpleNextTokenModel(vocab_size, embed_dim, hidden_dim)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4584d33e",
   "metadata": {},
   "source": [
    "## 训练数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64b1831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dummy_data(num_batches, batch_size, seq_len, vocab_size):\n",
    "    \"\"\"生成简单的随机数据\"\"\"\n",
    "    data = []\n",
    "    for _ in range(num_batches):\n",
    "        # 随机生成数据\n",
    "        batch = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "        data.append(batch)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efd4d0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, torch.Size([32, 129]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 128   # 序列长度\n",
    "batch_size = 32\n",
    "num_batches = 100 # 训练步数\n",
    "\n",
    "# 注意：这里的 data 包含输入和目标，所以 seq_len + 1\n",
    "train_data = generate_dummy_data(num_batches, batch_size, seq_len + 1, vocab_size)\n",
    "len(train_data), train_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343964b3",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "### CrossEntropyLoss 简介\n",
    "\n",
    "交叉熵损失（Cross Entropy Loss）衡量的是两个概率分布 $P$（真实分布）和 $Q$（预测分布）之间的差异。其公式为：\n",
    "\n",
    "$$\n",
    "H(P, Q) = -\\sum_{x} P(x) \\log Q(x)\n",
    "$$\n",
    "\n",
    "对于 Next Token Prediction 任务，真实分布 $P$ 是一个 **One-hot 向量**（只有真实的下一个 token 位置为 1，其余为 0）。假设真实 token 的索引是 $i$，那么公式简化为：\n",
    "\n",
    "$$\n",
    "Loss = - \\log(Q(x_i))\n",
    "$$\n",
    "\n",
    "其中 $Q(x_i)$ 是模型预测该位置为真实 token 的概率（经过 Softmax 归一化）。\n",
    "\n",
    "### 为什么大语言模型使用它？\n",
    "\n",
    "1. **最大似然估计（MLE）**：最小化交叉熵等价于最大化正确 token 的预测概率，这符合我们希望模型“猜对”下一个词的目标。\n",
    "2. **处理多分类问题**：LLM 的词表（Vocab Size）通常很大（数万到数十万），CrossEntropyLoss 能自然地处理这种大规模多分类问题。\n",
    "3. **梯度特性好**：当模型预测错误（概率低）时，$-\\log(p)$ 的梯度很大，能让模型迅速修正；当预测正确（概率接近 1）时，梯度趋近于 0，训练稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b41f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 定义 Loss 和 Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c595bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练，共 100 个 batch...\n",
      "Step [10/100], Loss: 6.9229\n",
      "Step [20/100], Loss: 6.9222\n",
      "Step [30/100], Loss: 6.9219\n",
      "Step [40/100], Loss: 6.9241\n",
      "Step [50/100], Loss: 6.9204\n",
      "Step [60/100], Loss: 6.9230\n",
      "Step [70/100], Loss: 6.9176\n",
      "Step [80/100], Loss: 6.9225\n",
      "Step [90/100], Loss: 6.9193\n",
      "Step [100/100], Loss: 6.9180\n",
      "训练结束！耗时: 7.11 秒\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# 记录开始时间\n",
    "start_time = time.time()\n",
    "print(f\"开始训练，共 {num_batches} 个 batch...\")\n",
    "model.train()\n",
    "\n",
    "for step, batch in enumerate(train_data):\n",
    "    # 构造输入和目标 (Next Token Prediction)\n",
    "    # 输入: 序列的前 N-1 个 token\n",
    "    # 目标: 序列的后 N-1 个 token (即每个位置的下一个 token)\n",
    "    input_ids = batch[:, :-1]  # (B, T)\n",
    "    targets = batch[:, 1:]     # (B, T)\n",
    "    \n",
    "    # 清零梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播\n",
    "    logits = model(input_ids) # (B, T, V)\n",
    "    \n",
    "    # 计算 Loss\n",
    "    # CrossEntropyLoss 需要 (N, C) 和 (N) 的输入\n",
    "    # logits.view(-1, vocab_size) -> (B*T, V)\n",
    "    # targets.view(-1) -> (B*T)\n",
    "    loss = criterion(logits.view(-1, vocab_size), targets.reshape(-1))\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新参数\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step + 1) % 10 == 0:\n",
    "        print(f\"Step [{step+1}/{num_batches}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"训练结束！耗时: {end_time - start_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798fc1f",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16b5d991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "验证第一个 batch 的预测:\n",
      "输入 shape: torch.Size([32, 128])\n",
      "预测 shape: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "# 简单验证一下\n",
    "test_input = train_data[0][:, :-1]\n",
    "with torch.no_grad():\n",
    "    logits = model(test_input)\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "    print(\"\\n验证第一个 batch 的预测:\")\n",
    "    print(f\"输入 shape: {test_input.shape}\")\n",
    "    print(f\"预测 shape: {preds.shape}\")\n",
    "    # 这里只是随机数据，预测准确率不会高，主要是验证流程跑通"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50604e24",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0be1b6",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
