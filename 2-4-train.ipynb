{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abc9b72",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "```{note}\n",
    "现在有了 model, tokenizer 和 dataset, 我们可以开始训练了。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea35d4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29883833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Imports from local files\n",
    "from torch_train.torch_tokenizer import BPETokenizer\n",
    "from torch_train.torch_model import TransformerModel\n",
    "from torch_train.torch_dataset import PretrainDataset\n",
    "\n",
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    \"parquet_path\": \"data/wikitext-103-raw-v1-train-sampled.parquet\", # Use sample set for lower resource usage\n",
    "    \"tokenizer_path\": \"wiki-tokenizer-1.json\",\n",
    "    \"seq_len\": 512,           # Reduced context window\n",
    "    \"batch_size\": 8,         # Reduced batch size\n",
    "    \n",
    "    # Model (Tiny size for stability)\n",
    "    \"d_model\": 256,          # Reduced d_model\n",
    "    \"n_head\": 8,\n",
    "    \"d_hidden\": 1024,         # Reduced d_hidden\n",
    "    \"n_layer\": 16,            # Reduced n_layer\n",
    "    \"dropout\": 0.1,\n",
    "    \n",
    "    # Training\n",
    "    \"lr\": 3e-4,\n",
    "    \"epochs\": 2,             # Increased epochs since data is smaller\n",
    "    \"log_interval\": 10,\n",
    "    \"save_path\": \"llm_checkpoint_27m.pt\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504af4b2",
   "metadata": {},
   "source": [
    "模型的参数量：\n",
    "1. 除 transformer block 外的参数数量：\n",
    "    *  embedding 层：`d_model * vocab_size`\n",
    "    *  output 层：`vocab_size * d_model`\n",
    "    *  RMSNorm：`d_model`\n",
    "2.  每个 transformer block 的参数数量：\n",
    "    *  attention 的参数量：\n",
    "        *  qkv 投影：`3 * d_model * d_model`\n",
    "        *  o 投影：`d_model * d_model`\n",
    "        *  总的和 QK 的 RMSNorm：`3 * d_model`\n",
    "    *  feed forward network 的参数量：\n",
    "        *  gate 和 up projection：`2 * d_model * d_hidden`\n",
    "        *  down projection：`d_hidden * d_model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1684481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27033856"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = CONFIG[\"d_model\"]\n",
    "vocab_size = 20000\n",
    "n_layer = CONFIG[\"n_layer\"]\n",
    "d_hidden = CONFIG[\"d_hidden\"]\n",
    "d_model * (2 * vocab_size + 1 ) + n_layer * (4 * d_model * d_model + 3 * d_model * d_hidden + 4 * d_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c192a",
   "metadata": {},
   "source": [
    "## 加载 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07dfabd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Tokenizer...\n",
      "Tokenizer loaded. Vocab size: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.load(CONFIG[\"tokenizer_path\"])\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "pad_id = tokenizer.special_tokens.get(\"<PAD>\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f97030",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a332d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/wikitext-103-raw-v1-train-sampled.parquet...\n",
      "Initializing Dataset...\n",
      "Processing 1 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1/1 [00:00<00:00,  2.86it/s]\n",
      "Tokenizing texts: 100%|██████████| 2944/2944 [01:01<00:00, 48.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 12880663. Total samples (seq_len=513): 25109\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from {CONFIG['parquet_path']}...\")\n",
    "file_paths = [CONFIG[\"parquet_path\"]] \n",
    "\n",
    "print(\"Initializing Dataset...\")\n",
    "# Request seq_len + 1 to handle input/target shift\n",
    "dataset = PretrainDataset(file_paths, tokenizer, seq_len=CONFIG[\"seq_len\"] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0e15453",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5b3c3",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12dff572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n",
      "Model parameters: 27.03M\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Model...\")\n",
    "model = TransformerModel(\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    n_head=CONFIG[\"n_head\"],\n",
    "    d_hidden=CONFIG[\"d_hidden\"],\n",
    "    n_layer=CONFIG[\"n_layer\"],\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=CONFIG[\"seq_len\"] + 1, \n",
    "    dropout=CONFIG[\"dropout\"]\n",
    ").to(CONFIG[\"device\"])\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1df992",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15641152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   6%|▋         | 200/3139 [00:50<12:25,  3.94batch/s, loss=6.3364, lr=2.99e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 200/3139 | Loss: 6.3364 | LR: 2.99e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  13%|█▎        | 400/3139 [01:40<11:23,  4.01batch/s, loss=5.9198, lr=2.97e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 400/3139 | Loss: 5.9198 | LR: 2.97e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  19%|█▉        | 600/3139 [02:31<10:44,  3.94batch/s, loss=5.6097, lr=2.93e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 600/3139 | Loss: 5.6097 | LR: 2.93e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  25%|██▌       | 800/3139 [03:21<09:52,  3.95batch/s, loss=5.2382, lr=2.88e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 800/3139 | Loss: 5.2382 | LR: 2.88e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  32%|███▏      | 1000/3139 [04:12<08:57,  3.98batch/s, loss=5.1867, lr=2.82e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1000/3139 | Loss: 5.1867 | LR: 2.82e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  38%|███▊      | 1200/3139 [05:02<07:59,  4.04batch/s, loss=5.0102, lr=2.74e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1200/3139 | Loss: 5.0102 | LR: 2.74e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  45%|████▍     | 1400/3139 [05:53<07:25,  3.91batch/s, loss=5.0341, lr=2.65e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1400/3139 | Loss: 5.0341 | LR: 2.65e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  51%|█████     | 1600/3139 [06:45<06:28,  3.96batch/s, loss=4.8956, lr=2.54e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1600/3139 | Loss: 4.8956 | LR: 2.54e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  57%|█████▋    | 1800/3139 [07:36<05:39,  3.95batch/s, loss=4.7985, lr=2.43e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1800/3139 | Loss: 4.7985 | LR: 2.43e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  64%|██████▎   | 2000/3139 [08:27<04:51,  3.91batch/s, loss=4.7320, lr=2.31e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 2000/3139 | Loss: 4.7320 | LR: 2.31e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  70%|███████   | 2200/3139 [09:17<03:57,  3.96batch/s, loss=4.7224, lr=2.18e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 2200/3139 | Loss: 4.7224 | LR: 2.18e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  76%|███████▋  | 2400/3139 [10:08<03:06,  3.96batch/s, loss=4.7545, lr=2.04e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 2400/3139 | Loss: 4.7545 | LR: 2.04e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  83%|████████▎ | 2600/3139 [10:59<02:15,  3.96batch/s, loss=4.7348, lr=1.90e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 2600/3139 | Loss: 4.7348 | LR: 1.90e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  89%|████████▉ | 2800/3139 [11:49<01:24,  4.02batch/s, loss=4.4990, lr=1.75e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 2800/3139 | Loss: 4.4990 | LR: 1.75e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  96%|█████████▌| 3000/3139 [12:40<00:35,  3.94batch/s, loss=4.5412, lr=1.60e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 3000/3139 | Loss: 4.5412 | LR: 1.60e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 3139/3139 [13:15<00:00,  3.95batch/s, loss=4.5805, lr=1.50e-04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Complete. Average Loss: 5.2000\n",
      "Checkpoint saved to llm_checkpoint_27m.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   6%|▋         | 200/3139 [00:50<12:23,  3.95batch/s, loss=4.6308, lr=1.35e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 200/3139 | Loss: 4.6308 | LR: 1.35e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  13%|█▎        | 400/3139 [01:41<11:39,  3.92batch/s, loss=4.3921, lr=1.20e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 400/3139 | Loss: 4.3921 | LR: 1.20e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  19%|█▉        | 600/3139 [02:31<10:38,  3.98batch/s, loss=4.5454, lr=1.06e-04]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 600/3139 | Loss: 4.5454 | LR: 1.06e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|██▌       | 800/3139 [03:22<09:50,  3.96batch/s, loss=4.3014, lr=9.15e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 800/3139 | Loss: 4.3014 | LR: 9.15e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  32%|███▏      | 1000/3139 [04:13<08:59,  3.96batch/s, loss=4.2931, lr=7.80e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 1000/3139 | Loss: 4.2931 | LR: 7.80e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  38%|███▊      | 1200/3139 [05:03<08:09,  3.96batch/s, loss=4.2559, lr=6.52e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 1200/3139 | Loss: 4.2559 | LR: 6.52e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  45%|████▍     | 1400/3139 [05:54<07:20,  3.95batch/s, loss=4.3630, lr=5.33e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 1400/3139 | Loss: 4.3630 | LR: 5.33e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  51%|█████     | 1600/3139 [06:44<06:31,  3.93batch/s, loss=4.1623, lr=4.23e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 1600/3139 | Loss: 4.1623 | LR: 4.23e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  57%|█████▋    | 1800/3139 [07:35<05:41,  3.92batch/s, loss=4.3556, lr=3.24e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 1800/3139 | Loss: 4.3556 | LR: 3.24e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  64%|██████▎   | 2000/3139 [08:26<04:48,  3.95batch/s, loss=4.1755, lr=2.37e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 2000/3139 | Loss: 4.1755 | LR: 2.37e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  70%|███████   | 2200/3139 [09:16<03:57,  3.96batch/s, loss=4.3754, lr=1.63e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 2200/3139 | Loss: 4.3754 | LR: 1.63e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  76%|███████▋  | 2400/3139 [10:07<03:04,  4.01batch/s, loss=4.0642, lr=1.01e-05]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 2400/3139 | Loss: 4.0642 | LR: 1.01e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  83%|████████▎ | 2600/3139 [10:58<02:16,  3.95batch/s, loss=4.7023, lr=5.42e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 2600/3139 | Loss: 4.7023 | LR: 5.42e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  89%|████████▉ | 2800/3139 [11:48<01:25,  3.97batch/s, loss=4.2579, lr=2.15e-06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 2800/3139 | Loss: 4.2579 | LR: 2.15e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  96%|█████████▌| 3000/3139 [12:39<00:34,  4.02batch/s, loss=4.3050, lr=3.63e-07]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Step 3000/3139 | Loss: 4.3050 | LR: 3.63e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 3139/3139 [13:14<00:00,  3.95batch/s, loss=4.3537, lr=0.00e+00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Complete. Average Loss: 4.3229\n",
      "Checkpoint saved to llm_checkpoint_27m.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "# Total steps for scheduler\n",
    "total_steps = len(dataloader) * CONFIG[\"epochs\"]\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    # 使用 tqdm 包装 dataloader\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\", unit=\"batch\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(pbar):\n",
    "        data = data.to(CONFIG[\"device\"])\n",
    "        \n",
    "        # Input: [B, seq_len], Target: [B, seq_len] (shifted)\n",
    "        input_ids = data[:, :-1]\n",
    "        target_ids = data[:, 1:]\n",
    "\n",
    "        # Generate position_ids\n",
    "        B, T = input_ids.shape\n",
    "        position_ids = torch.arange(T, device=CONFIG[\"device\"]).unsqueeze(0).expand(B, T)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, position_ids)\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, vocab_size), target_ids.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条上的 loss 和 LR\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"lr\": f\"{current_lr:.2e}\"})\n",
    "        \n",
    "        # 每 200 step 打印一次 loss\n",
    "        if (batch_idx + 1) % 200 == 0:\n",
    "             print(f\"Epoch {epoch+1} | Step {batch_idx+1}/{len(dataloader)} | Loss: {loss.item():.4f} | LR: {current_lr:.2e}\")\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} Complete. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), CONFIG[\"save_path\"])\n",
    "    print(f\"Checkpoint saved to {CONFIG['save_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27520a3b",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eval_ppl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from data/wikitext-103-raw-v1-test.parquet...\n",
      "Processing 1 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1/1 [00:00<00:00, 117.19it/s]\n",
      "Tokenizing texts: 100%|██████████| 62/62 [00:01<00:00, 40.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 304473. Total samples (seq_len=513): 594\n",
      "Calculating Perplexity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 75/75 [00:06<00:00, 11.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.4311\n",
      "Test Perplexity: 84.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# 在测试集上进行评估\n",
    "test_path = \"data/wikitext-103-raw-v1-test.parquet\"\n",
    "print(f\"Loading test data from {test_path}...\")\n",
    "test_file_paths = [test_path]\n",
    "test_dataset = PretrainDataset(test_file_paths, tokenizer, seq_len=CONFIG[\"seq_len\"] + 1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=2)\n",
    "\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "    \n",
    "    print(\"Calculating Perplexity...\")\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            data = data.to(device)\n",
    "            input_ids = data[:, :-1]\n",
    "            target_ids = data[:, 1:]\n",
    "            \n",
    "            B, T = input_ids.shape\n",
    "            position_ids = torch.arange(T, device=device).unsqueeze(0).expand(B, T)\n",
    "            \n",
    "            output = model(input_ids, position_ids)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target_ids.reshape(-1))\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "            \n",
    "    avg_loss = total_loss / total_steps\n",
    "    ppl = math.exp(avg_loss)\n",
    "    return avg_loss, ppl\n",
    "\n",
    "test_loss, test_ppl = calculate_perplexity(model, test_dataloader, CONFIG[\"device\"])\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inference_demo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The weather is\n",
      "----------------------------------------\n",
      "The weather is a tropical cyclone that is a tropical cyclone , and is a tropical cyclone that is a tropical storm . The storm is a tropical storm , and is a tropical storm . The storm is a tropical storm , and the storm is a tropical storm . The storm\n",
      "========================================\n",
      "Prompt: It was a\n",
      "----------------------------------------\n",
      "It was a member of the Royal Navy . The ship was a ship of the ship 's main armament , and the ship was built in the Atlantic Ocean . The ship was built in the Pacific Ocean , and the ship was built in the Atlantic Ocean . The ship\n",
      "========================================\n",
      "Prompt: In the early\n",
      "----------------------------------------\n",
      "In the early 20th century . The first of the first two @-@ third @-@ century @-@ century @-@ style castles were the first of the first two @-@ third @-@ century American , the first of the first two @-@ third @-@ century American , the first of the first two\n",
      "========================================\n",
      "Prompt: The game began with\n",
      "----------------------------------------\n",
      "The game began with a new game on September 2 , 2008 . The game was released on September 2 , 2010 , and was released on November 2 , 2012 . The game was released on October 2 , 2010 , and was released on November 2 , 2012 . The game was\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    input_tensor = torch.tensor(input_ids, dtype=torch.long, device=CONFIG[\"device\"]).unsqueeze(0)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    generated = input_ids.copy()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # 2. Crop context if too long (keep only the last seq_len tokens)\n",
    "            ctx = input_tensor[:, -CONFIG[\"seq_len\"]:]\n",
    "            \n",
    "            # 3. Create position IDs\n",
    "            B, T = ctx.shape\n",
    "            position_ids = torch.arange(T, device=CONFIG[\"device\"]).unsqueeze(0).expand(B, T)\n",
    "            \n",
    "            # 4. Forward pass\n",
    "            output = model(ctx, position_ids)\n",
    "            \n",
    "            # 5. Get the logits for the last token\n",
    "            next_token_logits = output[0, -1, :]\n",
    "            \n",
    "            # 6. Greedy decoding: choose the token with the highest probability\n",
    "            next_token_id = torch.argmax(next_token_logits).item()\n",
    "            \n",
    "            # 7. Append the predicted token\n",
    "            generated.append(next_token_id)\n",
    "            input_tensor = torch.cat((input_tensor, torch.tensor([[next_token_id]], device=CONFIG[\"device\"])), dim=1)\n",
    "            \n",
    "            # 8. Stop if <EOS> is generated\n",
    "            if next_token_id == tokenizer.special_tokens.get(\"<EOS>\"):\n",
    "                break\n",
    "                \n",
    "    # 9. Decode the generated tokens to string\n",
    "    decoded = tokenizer.decode(generated)\n",
    "    print(decoded)\n",
    "    print(\"=\" * 40)\n",
    "    return decoded\n",
    "\n",
    "# Sample prompts\n",
    "prompts = [\n",
    "    \"The weather is\",\n",
    "    \"It was a\",\n",
    "    \"In the early\",\n",
    "    \"The game began with\"\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    generate(model, tokenizer, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c87c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
