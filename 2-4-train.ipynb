{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abc9b72",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "```{note}\n",
    "现在有了 model, tokenizer 和 dataset, 我们可以开始训练了。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bea35d4",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29883833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Imports from local files\n",
    "from torch_train.torch_tokenizer import BPETokenizer\n",
    "from torch_train.torch_model import TransformerModel\n",
    "from torch_train.torch_dataset import PretrainDataset\n",
    "\n",
    "# ==========================================\n",
    "# Configuration\n",
    "# ==========================================\n",
    "CONFIG = {\n",
    "    # Data\n",
    "    \"parquet_path\": \"data/wikitext-103-raw-v1-train.parquet\", # Use train set for lower resource usage\n",
    "    \"tokenizer_path\": \"wiki-tokenizer-1.json\",\n",
    "    \"seq_len\": 512,           # Reduced context window\n",
    "    \"batch_size\": 8,         # Reduced batch size\n",
    "    \n",
    "    # Model (Tiny size for stability)\n",
    "    \"d_model\": 512,          # Reduced d_model\n",
    "    \"n_head\": 8,\n",
    "    \"d_hidden\": 2048,         # Reduced d_hidden\n",
    "    \"n_layer\": 16,            # Reduced n_layer\n",
    "    \"dropout\": 0.1,\n",
    "    \n",
    "    # Training\n",
    "    \"lr\": 3e-4,\n",
    "    \"epochs\": 2,             # Increased epochs since data is smaller\n",
    "    \"log_interval\": 10,\n",
    "    \"save_path\": \"llm_checkpoint.pt\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504af4b2",
   "metadata": {},
   "source": [
    "模型的参数量：\n",
    "1. 除 transformer block 外的参数数量：\n",
    "    *  embedding 层：`d_model * vocab_size`\n",
    "    *  output 层：`vocab_size * d_model`\n",
    "    *  RMSNorm：`d_model`\n",
    "2.  每个 transformer block 的参数数量：\n",
    "    *  attention 的参数量：\n",
    "        *  qkv 投影：`3 * d_model * d_model`\n",
    "        *  o 投影：`d_model * d_model`\n",
    "        *  总的和 QK 的 RMSNorm：`3 * d_model`\n",
    "    *  feed forward network 的参数量：\n",
    "        *  gate 和 up projection：`2 * d_model * d_hidden`\n",
    "        *  down projection：`d_hidden * d_model`\n",
    "\n",
    "$$\n",
    "\\text{参数量} = d\\_model * (2 * vocab\\_size + 1) + n\\_layer * (4 * d\\_model * d\\_model + 3 * d\\_model * d\\_hidden + 4 * d\\_model)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1684481a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87622144"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512 * (2 * 20000 + 1) + 16 * (4 * 512 * 512 + 3 * 512 * 2048 + 4 * 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c192a",
   "metadata": {},
   "source": [
    "## 加载 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07dfabd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading Tokenizer...\n",
      "Tokenizer loaded. Vocab size: 20000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {CONFIG['device']}\")\n",
    "\n",
    "print(\"Loading Tokenizer...\")\n",
    "tokenizer = BPETokenizer()\n",
    "if os.path.exists(CONFIG[\"tokenizer_path\"]):\n",
    "    tokenizer.load(CONFIG[\"tokenizer_path\"])\n",
    "    print(f\"Tokenizer loaded. Vocab size: {len(tokenizer.vocab)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Tokenizer file not found at {CONFIG['tokenizer_path']}\")\n",
    "\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "pad_id = tokenizer.special_tokens.get(\"<PAD>\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f97030",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a332d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading data from {CONFIG['parquet_path']}...\")\n",
    "file_paths = [CONFIG[\"parquet_path\"]] \n",
    "\n",
    "print(\"Initializing Dataset...\")\n",
    "# Request seq_len + 1 to handle input/target shift\n",
    "dataset = PretrainDataset(file_paths, tokenizer, seq_len=CONFIG[\"seq_len\"] + 1)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5b3c3",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12dff572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model...\n",
      "Model parameters: 87.61M\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing Model...\")\n",
    "model = TransformerModel(\n",
    "    d_model=CONFIG[\"d_model\"],\n",
    "    n_head=CONFIG[\"n_head\"],\n",
    "    d_hidden=CONFIG[\"d_hidden\"],\n",
    "    n_layer=CONFIG[\"n_layer\"],\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=CONFIG[\"seq_len\"] + 1, \n",
    "    dropout=CONFIG[\"dropout\"]\n",
    ").to(CONFIG[\"device\"])\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1df992",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15641152",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(CONFIG[\"epochs\"]):\n",
    "    total_loss = 0\n",
    "    # 使用 tqdm 包装 dataloader\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']}\", unit=\"batch\")\n",
    "    \n",
    "    for batch_idx, data in enumerate(pbar):\n",
    "        data = data.to(CONFIG[\"device\"])\n",
    "        \n",
    "        # Input: [B, seq_len], Target: [B, seq_len] (shifted)\n",
    "        input_ids = data[:, :-1]\n",
    "        target_ids = data[:, 1:]\n",
    "\n",
    "        # Generate position_ids\n",
    "        B, T = input_ids.shape\n",
    "        position_ids = torch.arange(T, device=CONFIG[\"device\"]).unsqueeze(0).expand(B, T)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, position_ids)\n",
    "        \n",
    "        loss = criterion(output.reshape(-1, vocab_size), target_ids.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # 更新进度条上的 loss\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "            \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} Complete. Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), CONFIG[\"save_path\"])\n",
    "    print(f\"Checkpoint saved to {CONFIG['save_path']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
